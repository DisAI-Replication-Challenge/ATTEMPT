[[configs]]
datasets = ["squad"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.01
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_length = 64
num_epochs = 20
batch_size = 16
peft_type = "prompt_tunning"
task_type = "seq_2_seq_lm"
num_virtual_tokens = 100
n_runs = 10
wandb_project = "prompt_tunning_soft_prompt_experiments"
max_source_length = 256


[[configs]]
datasets = ["mrpc"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.01
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_length = 64
num_epochs = 20
batch_size = 16
peft_type = "prompt_tunning"
task_type = "seq_2_seq_lm"
num_virtual_tokens = 100
n_runs = 10
wandb_project = "prompt_tunning_soft_prompt_experiments"
max_source_length = 256

[[configs]]
datasets = ["sst2"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.01
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_length = 64
num_epochs = 20
batch_size = 16
peft_type = "prompt_tunning"
task_type = "seq_2_seq_lm"
num_virtual_tokens = 100
n_runs = 10
wandb_project = "prompt_tunning_soft_prompt_experiments"
max_source_length = 256

[[configs]]
datasets = ["qnli"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.01
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_length = 64
num_epochs = 20
batch_size = 16
peft_type = "prompt_tunning"
task_type = "seq_2_seq_lm"
num_virtual_tokens = 100
n_runs = 10
wandb_project = "prompt_tunning_soft_prompt_experiments"
max_source_length = 256


[[configs]]
datasets = ["mnli"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.01
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_length = 64
num_epochs = 20
batch_size = 16
peft_type = "prompt_tunning"
task_type = "seq_2_seq_lm"
num_virtual_tokens = 100
n_runs = 10
wandb_project = "prompt_tunning_soft_prompt_experiments"
max_source_length = 256

[[configs]]
datasets = ["qqp"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.01
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_length = 64
num_epochs = 20
batch_size = 16
peft_type = "prompt_tunning"
task_type = "seq_2_seq_lm"
num_virtual_tokens = 100
n_runs = 10
wandb_project = "prompt_tunning_soft_prompt_experiments"
max_source_length = 256

[[configs]]
datasets = ["superglue-record"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.01
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_length = 64
num_epochs = 20
batch_size = 16
peft_type = "prompt_tunning"
task_type = "seq_2_seq_lm"
num_virtual_tokens = 100
n_runs = 10
wandb_project = "prompt_tunning_soft_prompt_experiments"
max_source_length = 256