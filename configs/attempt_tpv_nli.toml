[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 0
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 0
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = false

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 0
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 0
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = false

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 0
batch_size = 32
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 0
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = false

# 5-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 5
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 5
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 5
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

# 10-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 10
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 10
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 10
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

# 25-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 25
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 25
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 30
batch_size = 2
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 25
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

# 50-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 50
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 50
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 50
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

# 100-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 100
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 100
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 100
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

# 250-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 250
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 250
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 20
batch_size = 8
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 250
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

# 500-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 500
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 500
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 500
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

# 750-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 750
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 750
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 750
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

# 1000-shots
[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_0/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_0/mnli.bin", "soft_prompts/origin_0/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 1000
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_1/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_1/mnli.bin", "soft_prompts/origin_1/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 1000
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true

[[configs]]
datasets = ["snli", "scitail"]
dataset_config_name = ["en"]
device = "cuda"
learning_rate = 0.3
model_name_or_path = "t5-base"
tokenizer_name_or_path = "t5-base"
max_target_length = 128
num_epochs = 10
batch_size = 16
peft_type = "attempt"
attn_method = "sub"
temperature = 2087
prompt_init = "embedding_multi"
prompt_init_embedding = "soft_prompts/origin_2/mnli.bin"
prompt_embedding_paths = ["soft_prompts/origin_2/mnli.bin", "soft_prompts/origin_2/qnli.bin"]
prefix_num=2
task_type = "seq_2_seq_lm"
num_virtual_tokens = 50
n_runs = 1
wandb_project = "attempt_multi_nli"
max_source_length = 256
split_validation_test = true
max_train_samples = 1000
max_valid_samples = 250
output_dir = "attempt_multi_nli"
warmup_steps = 500
shared_attn = true
pad_to_max_length = true
load_model = true